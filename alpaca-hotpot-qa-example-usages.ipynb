{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222facc6-d05a-4b4d-b91d-8d28f25c0a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import alpaca_hotpot_qa\n",
    "import transformers\n",
    "from peft import PeftModel\n",
    "from transformers import GenerationConfig, LlamaForCausalLM, LlamaTokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcc0739-9790-44f2-9044-4d4d6d07fcd4",
   "metadata": {},
   "source": [
    "## Load Alpaca model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049bbece-ff9f-458a-94fa-0ca12da8898f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8786402b-135e-491c-bfb4-3cafde7b439e",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_8bit = False\n",
    "base_model = 'decapoda-research/llama-7b-hf'\n",
    "lora_weights = 'tloen/alpaca-lora-7b'\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
    "if device == \"cuda\":\n",
    "    alp_model = LlamaForCausalLM.from_pretrained(base_model, load_in_8bit=load_8bit,\n",
    "                    torch_dtype=torch.float16, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d640cce-fccf-4cfd-90c7-8a1052f071f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if can't find xxx.json error occurred, check the lora_weights variable, trying using full system path\n",
    "\n",
    "if device == \"cuda\":\n",
    "    alp_model = PeftModel.from_pretrained(alp_model, lora_weights, torch_dtype=torch.float16)\n",
    "\n",
    "# unwind broken decapoda-research config\n",
    "alp_model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\n",
    "alp_model.config.bos_token_id = 1\n",
    "alp_model.config.eos_token_id = 2\n",
    "\n",
    "if not load_8bit:\n",
    "    alp_model.half()  # seems to fix bugs for some users.\n",
    "\n",
    "alp_model.eval()\n",
    "if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "    alp_model = torch.compile(alp_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7a3ee8-5f4f-43a5-aa39-64975fbb24e5",
   "metadata": {},
   "source": [
    "## Load phone database pickle file\n",
    "\n",
    "(phone_dataset.pkl was a bad naming, it should have been phone_database.pkl instead, but it's too time-consuming to rerun everything that uses this file, so just keeping it as it is)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85afa5d8-fc05-496c-a9da-a161b40e6e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#load phonedb data\n",
    "with open(\"phone_dataset.pkl\", \"rb\") as f:\n",
    "    pdb = pickle.load(f)\n",
    "phonedb_data, name_map = pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1586db8d-c249-48bd-9374-3833720d5984",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ab6862f-cd5b-44fe-9d3f-68487d49111b",
   "metadata": {},
   "source": [
    "## Import the Alpaca-Hotpot model (a.k.a Alpaca-Fusion model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e49839-b1da-41e7-b948-395fafaa8911",
   "metadata": {},
   "outputs": [],
   "source": [
    "from alpaca_hotpot_qa import AlpacaHotPotQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14750bfd-e5b6-4b45-aae8-ab22b92b797d",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphot_qa_inference = AlpacaHotPotQA(device, alp_model, tokenizer, phonedb_data, name_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674e0afc-d72e-4dce-ad8b-2be54851af38",
   "metadata": {},
   "source": [
    "## Perform inference\n",
    "Everything was wrapped in alpaca_hotpot_qa.py, simply pass the question as the function argument, the model inference method will return an answer, the default parameter for print_process is true, hence it will print the internal inference results of each sub-models in the Alpaca-fusioning with other classical NLP models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4b2941-e26b-4e52-b6f2-574aab1d99cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = '''\\\n",
    "\"How do the camera capabilities of the Apple iPhone 12, Samsung Galaxy S21, and Xiaomi Mi 11 compare?\"\n",
    "'''\n",
    "answer = alphot_qa_inference.inference(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
